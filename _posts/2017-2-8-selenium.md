---
layout: post
title: How to Scrape the Web using Python
---

A common programming task is scraping a web page, downloading it for processing. This can be useful for collecting information from a website doesn't have an API or web automation.

### First attempt: urllib.request
Python programmers will know that downloading a webpage isn't particularly difficult in fact it's only two lines using Python urllib library:

<pre><code>    import urllib.request
     html = urllib.request.urlopen("http://www.google.com")
</code></pre>

That wasn't so bad. So we're done right? Well not quite, this can't download any content generated by JavaScript which is becoming increasingly common nowadays.

### Second attempt: selenium

[Selenium](http://selenium-python.readthedocs.io/) is a browser automation tool whose most common application is to test website functionality (e.g. does an element appear when a user presses a button). It's great for this purpose but web-scraping? Not so much...

<pre><code>    import signals
    from selenium import webdriver
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException
    from selenium.webdriver.common.by import By

    page_hook = "//div[@id='#examplehook']"
    browser = webdriver.PhantomJS(service_args=['--load-images=no'])
    browser.get("http://www.google.com")

    # Wait until a particular XPATH is loaded
    try:
       element_presence = EC.presence_of_element_located(page_hook)
       WebDriverWait(browser, 10).until(element_presence)
       response = parse_callback(url, browser.page_source)
    except TimeoutException:
         pass

    browser.service.process.send_signal(signal.SIGTERM)
    browser.quit()
</code></pre>

The bulk of this new code is to wait up to 10 seconds for a element to appear on the page. The reason this is required is because of the callback nature of JavaScript it is practically impossible to know when a page is actually done loading. There could be a timeout event that triggers 10 minutes after the code starts executing, for example. But for most cases simply checking for the presence of elements `EC.presence_of_element_located(page_hook)` is good enough.

#### Selenium Scraping Pros

* Great versatility (can be used to fill forms too!)
* Simpler than a custom implementation

#### Selenium Scraping Cons

* The PhantomJS subprocess never shuts down even after quit(). A SIGTERM is required.
* Many imports are required for a simple task
* It is slowwww (~50% more than the next solution)



### Third attempt: dryscrape

[Dryscrape](https://dryscrape.readthedocs.io/en/latest/) is a thin python wrapper on [QT's webkit](http://doc.qt.io/qt-4.8/qtwebkit-module.html). It's use should be as simple as `sess.visit` and waiting for an element to be present on the page, but unfortunately, like selenium, it doesn't properly close the server process. After some digging in the source code, I found that we can manually pass a server instance and kill it ourselves after we are done with it, fixing this flaw.

<pre><code>    svr = webkit_server.Server()
    svrconn = webkit_server.ServerConnection(server=svr)
    driver = dryscrape.driver.webkit.Driver(connection=svrconn)

    sess = dryscrape.Session(driver=driver)
    sess.set_attribute("auto_load_images", False)

    page_hook = "//div[@id='#examplehook']"
    valid_page_func = lambda: sess.at_xpath(page_hook)

    sess.visit(url)
    try:
        sess.wait_for(valid_page_func, interval=1, timeout=10)
        response = sess.body()
    except dryscrape.mixins.WaitTimeoutError:
        pass

    sess.reset()
    svr.kill()
    db.destroy()
</code></pre>

#### Dryscrape Scraping Pros

* Faster than selenium as it uses Qt's C++ Webkit implementation
* Simpler than the alternatives

#### Dryscrape Scraping Cons

* Still requires a workaround to properly end the process

### Conclusion

It is surprising that such a seemingly simple task has so many approaches that require some workarounds, but nonetheless I'm glad that libraries like dryscrape exist so I don't have to get my hands dirty messing with lower level libraries. Happy scraping!
